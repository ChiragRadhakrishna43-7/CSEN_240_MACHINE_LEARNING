{"cells":[{"cell_type":"markdown","metadata":{"id":"JmeTNIkv_U3J"},"source":["# Simple CNN for MNIST \n"]},{"cell_type":"markdown","metadata":{"id":"D-xWqYId_U3M"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fx2kxhdh_U3N"},"outputs":[],"source":["import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{"id":"dq6m6gFl_U3O"},"source":["## Prepare the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50ZpbPVg_U3O"},"outputs":[],"source":["# Model / data parameters\n","num_classes = 10\n","input_shape = (28, 28, 1)\n","\n","# Load the data and split it between train and test sets\n","(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","\n","# Scale images to the [0, 1] range\n","x_train = x_train.astype(\"float32\") / 255\n","x_test = x_test.astype(\"float32\") / 255\n","# Make sure images have shape (28, 28, 1)\n","x_train = np.expand_dims(x_train, -1)\n","x_test = np.expand_dims(x_test, -1)\n","print(\"x_train shape:\", x_train.shape)\n","print(x_train.shape[0], \"train samples\")\n","print(x_test.shape[0], \"test samples\")\n","\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"]},{"cell_type":"markdown","metadata":{"id":"cMBBjbgD_U3P"},"source":["## Build the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBoQzBB9_U3Q"},"outputs":[],"source":["model = keras.Sequential(\n","    [\n","        # To-do\n","        keras.Input(shape=input_shape),  # Define the input shape of the model\n","        # Add a convolutional layer with 8 filters and ReLU activation\n","        # Add a max pooling layer with pool size 2x2\n","        # Add another convolutional layer with 16 filters and ReLU activation\n","        # Add another max pooling layer with pool size 2x2\n","        # Flatten the 2D feature maps into a 1D vector\n","        # Apply dropout regularization with a rate of 0.5\n","        # Add a dense (fully connected) layer with softmax activation for classification\n","    ]\n",")\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"3_cFZMbv_U3Q"},"source":["## Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96th78xc_U3R"},"outputs":[],"source":["batch_size = 128  # Number of samples per gradient update\n","epochs = 15  # Number of times to iterate over the entire training dataset\n","\n","# Configure the model for training\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])  \n","\n","# Train the model on the training data\n","model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n","# x_train: input training data\n","# y_train: target training data\n","# batch_size: number of samples per gradient update\n","# epochs: number of times to iterate over the entire training dataset\n","# validation_split: the fraction of the training data to be used as validation data during training\n"]},{"cell_type":"markdown","metadata":{"id":"mN_ag1mj_U3R"},"source":["## Evaluate the trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DyaKNs9_U3S"},"outputs":[],"source":["score = model.evaluate(x_test, y_test, verbose=0)\n","print(\"Test accuracy:\", score[1])"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}