<h1>KNN Algorithm:</h1>
<p align="justify">a. KNN assumes that points close to each other in the space are in the same class. <br/>
b. Feature scaling is important in KNN because the distance metric used may be sensitive to the scale of the features. Therefore, it is common to normalize or standardize the features before applying KNN.<br/>
c. The K-value in KNN specifies the number of nearest neighbors to consider when making a prediction. This value is typically chosen based on cross-validation or other optimization techniques.<br/>
d. KNN uses a majority voting scheme to classify new data points. That is, it assigns the class label that is most common among the k-nearest neighbors.<br/>
e. It may not perform as well as other algorithms in high-dimensional spaces due to the "curse of dimensionality‚Äù.<br/>
f. KNN is not robust to irrelevant features.</p>

<h1>Bayesian Classifier</h1>
<p align="justify">Bayesian classifiers are probabilistic models that estimate the probability of each class label given the observed features using Bayesian theorem.<br/>
b. Bayesian classifiers assume that the features are conditionally independent given the class label.<br/>
c. While this assumption may not always hold in practice, Bayesian classifiers are still effective in many classification tasks.<br/>
d. Training Bayesian Classifier for continuous attributes involves computing the Probability Density Function (PDF). <br/>
e. Bayesian classifiers can produce probability features for each class label, which can be useful for decision-making and uncertainty analysis. </p>
